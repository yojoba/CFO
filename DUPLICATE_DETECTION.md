# ğŸ” DÃ©tection de Duplicates - Documentation

## Vue d'ensemble

Le systÃ¨me AgentCFO dÃ©tecte automatiquement les documents en double (duplicates) lors de l'upload, utilisant 3 stratÃ©gies complÃ©mentaires.

## ğŸ¯ StratÃ©gies de DÃ©tection

### 1. **Hash Exact du Fichier** (100% de similaritÃ©)
- Calcule le SHA256 du contenu du fichier
- DÃ©tecte les fichiers **exactement identiques**
- MÃªme si le nom est diffÃ©rent
- **Le plus rapide et le plus fiable**

### 2. **SimilaritÃ© du Contenu** (>85% de similaritÃ©)
- Utilise les embeddings vectoriels (pgvector)
- DÃ©tecte les documents au **contenu trÃ¨s similaire**
- MÃªme si le fichier est lÃ©gÃ¨rement diffÃ©rent (scan vs PDF original)
- Compare le texte extrait par OCR

### 3. **MÃ©tadonnÃ©es Similaires** (>85% de similaritÃ©)
- Compare : montant + type + date (Â±30 jours)
- DÃ©tecte les **mÃªmes factures/reÃ§us**
- Exemple : mÃªme facture uploadÃ©e 2 fois
- Utile pour documents similaires mais pas identiques

## ğŸ”„ Processus Automatique

Lors de l'upload d'un document :

```
1. Upload fichier
2. Calcul hash SHA256
3. Extraction OCR
4. Analyse IA
5. GÃ©nÃ©ration embeddings
6. âš ï¸ DÃ‰TECTION DUPLICATES
   â”œâ”€ Check hash exact
   â”œâ”€ Check similaritÃ© contenu
   â””â”€ Check mÃ©tadonnÃ©es
7. Marquage si duplicate dÃ©tectÃ©
8. Sauvegarde en base
```

## ğŸ“Š Champs de Base de DonnÃ©es

| Champ | Type | Description |
|-------|------|-------------|
| `file_hash` | String | Hash SHA256 du contenu du fichier |
| `is_duplicate` | Boolean | True si document est un doublon |
| `duplicate_of_id` | Integer | ID du document original |
| `similarity_score` | Float | Score de similaritÃ© (0.0-1.0) |

## ğŸŒ Nouveaux Endpoints API

### GET `/api/documents/duplicates`
Retourne tous les documents marquÃ©s comme duplicates

**RÃ©ponse** :
```json
[
  {
    "id": 10,
    "display_name": "Facture Ã‰lectricitÃ© Nov 2024",
    "is_duplicate": true,
    "duplicate_of_id": 5,
    "similarity_score": 0.98,
    "created_at": "2024-12-04T14:00:00Z"
  }
]
```

## ğŸ¨ Affichage Frontend

### Badge Duplicate
Les documents duplicates sont affichÃ©s avec :
- ğŸŸ¡ **Fond jaune clair** pour toute la ligne
- ğŸ“‹ **IcÃ´ne "Copy"** avec message
- **Pourcentage de similaritÃ©** : "Doublon dÃ©tectÃ© (98% similaire)"

### Exemple d'Affichage
```
ğŸ“„ Facture Ã‰lectricitÃ© Romande - Nov 2024
   ğŸ“‹ Doublon dÃ©tectÃ© (98% similaire)
   WhatsApp Image 2025-12-03 at 15.27.40.jpeg
```

## ğŸ§ª Tests et Exemples

### Test 1 : Upload du MÃªme Fichier
```bash
# Upload document.pdf
# Upload document.pdf (mÃªme fichier, mÃªme nom)
# RÃ©sultat : âš ï¸ Duplicate dÃ©tectÃ© (100% - hash exact)
```

### Test 2 : Upload Scan et PDF Original
```bash
# Upload facture_scan.jpg (scan d'une facture)
# Upload facture.pdf (PDF original de la mÃªme facture)
# RÃ©sultat : âš ï¸ Duplicate dÃ©tectÃ© (95% - similaritÃ© contenu)
```

### Test 3 : MÃªme Facture, Noms DiffÃ©rents
```bash
# Upload "Invoice_Nov.pdf" (facture 245 CHF du 15/11)
# Upload "Facture_Novembre.pdf" (mÃªme facture, mÃªme montant)
# RÃ©sultat : âš ï¸ Duplicate dÃ©tectÃ© (90% - mÃ©tadonnÃ©es similaires)
```

## âš™ï¸ Configuration

### Seuils de DÃ©tection

Les seuils par dÃ©faut sont dÃ©finis dans `DuplicateDetectionService` :

```python
exact_match_threshold = 0.99          # Hash exact
high_similarity_threshold = 0.95      # TrÃ¨s similaire
moderate_similarity_threshold = 0.85  # Probablement duplicate
metadata_match_window_days = 30       # Â±30 jours pour dates
```

### Ajuster les Seuils

Modifiez dans `backend/app/services/duplicate_detection_service.py` :

```python
def __init__(self):
    self.moderate_similarity_threshold = 0.80  # Plus permissif
    self.metadata_match_window_days = 60       # FenÃªtre plus large
```

## ğŸ”§ Utilisation

### Automatique
La dÃ©tection est **automatique** pour chaque upload. Aucune action requise.

### Manuel - VÃ©rifier les Duplicates
```bash
# Via API
curl http://localhost:8001/api/documents/duplicates \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### Supprimer les Duplicates
```bash
# 1. Identifier les duplicates
GET /api/documents/duplicates

# 2. Supprimer ceux qui ne sont pas nÃ©cessaires
DELETE /api/documents/{duplicate_id}
```

## ğŸ“ˆ Statistiques

Le endpoint `/api/documents/statistics` inclut maintenant :

```json
{
  "total_documents": 10,
  "duplicate_documents": 2,
  ...
}
```

## ğŸš¨ Comportement

### Quand un Duplicate est DÃ©tectÃ©

1. âœ… Le document est **quand mÃªme sauvegardÃ©**
2. âš ï¸ Il est **marquÃ© comme duplicate**
3. ğŸ”— **Lien vers l'original** (duplicate_of_id)
4. ğŸ“Š **Score de similaritÃ©** enregistrÃ©
5. ğŸ¨ **Affichage visuel** dans le frontend (fond jaune)

### L'Utilisateur Peut

- âœ… **Garder** le duplicate (si nÃ©cessaire)
- âœ… **Supprimer** le duplicate
- âœ… **Voir l'original** via le lien

## ğŸ’¡ Cas d'Usage

### Cas 1 : MÃªme Facture UploadÃ©e 2 Fois
```
Upload 1 : "facture_nov.pdf" â†’ OK
Upload 2 : "facture_nov.pdf" â†’ âš ï¸ Duplicate (hash 100%)
Action : Supprimer le duplicate
```

### Cas 2 : Scan et PDF Original
```
Upload 1 : "facture_scan.jpg" â†’ OK
Upload 2 : "facture_original.pdf" â†’ âš ï¸ Duplicate (contenu 96%)
Action : Garder le PDF (meilleure qualitÃ©), supprimer le scan
```

### Cas 3 : Documents Similaires mais DiffÃ©rents
```
Upload 1 : "Facture Ã‰lectricitÃ© Nov 2024" â†’ OK
Upload 2 : "Facture Ã‰lectricitÃ© Dec 2024" â†’ OK (dates diffÃ©rentes)
RÃ©sultat : Pas de duplicate (dates hors fenÃªtre de 30j)
```

## ğŸ” Debugging

### Voir les Logs de DÃ©tection
```bash
docker-compose logs backend | grep -i "duplicate"
```

Exemple de logs :
```
âš ï¸ Duplicate detected for document 10: original=5, similarity=0.98, method=exact_hash
âœ… No duplicate found for document 11
```

### VÃ©rifier en Base de DonnÃ©es
```sql
-- Voir tous les duplicates
SELECT 
    id, 
    display_name, 
    is_duplicate, 
    duplicate_of_id, 
    similarity_score 
FROM documents 
WHERE is_duplicate = true;

-- Voir les documents avec leurs duplicates
SELECT 
    d1.id as original_id,
    d1.display_name as original_name,
    d2.id as duplicate_id,
    d2.display_name as duplicate_name,
    d2.similarity_score
FROM documents d1
JOIN documents d2 ON d1.id = d2.duplicate_of_id
ORDER BY d1.id;
```

## ğŸ¯ Avantages

- âœ… **Ã‰vite le stockage inutile** de fichiers identiques
- âœ… **Alerte visuelle** immÃ©diate
- âœ… **Multiple stratÃ©gies** pour diffÃ©rents cas
- âœ… **DÃ©tection automatique** sans intervention
- âœ… **Flexible** : l'utilisateur dÃ©cide de garder ou supprimer

## âš™ï¸ Performance

- **Hash calculation** : ~50-200ms
- **Content similarity** : ~100-500ms
- **Metadata check** : ~10-50ms
- **Total overhead** : ~160-750ms par document

Le coÃ»t est minimal et la dÃ©tection se fait en arriÃ¨re-plan.

## ğŸ”„ AmÃ©liorations Futures

- [ ] Onglet "Duplicates" dans le frontend
- [ ] Action "Fusionner duplicates"
- [ ] Notification push lors de dÃ©tection
- [ ] PrÃ©vention d'upload si duplicate exact
- [ ] Suggestion de suppression automatique
- [ ] ML pour amÃ©liorer la dÃ©tection

## ğŸ“š Ressources

- Service : `backend/app/services/duplicate_detection_service.py`
- ModÃ¨le : `backend/app/models/document.py` (champs duplicate)
- Migration : `backend/migrations/003_add_duplicate_detection.sql`
- Frontend : `frontend/src/components/DocumentList.tsx` (affichage)

---

**Date d'implÃ©mentation** : 4 dÃ©cembre 2024  
**Version** : 1.0.0  
**Statut** : âœ… OPÃ‰RATIONNEL

